+++
title = "Epistemic and Aleatoric Uncertainty"
date = 2026-02-13
draft = true
+++

- RND
  - curiosity
  - state, action, reward, done
- Noisy TV Problem
- Information theory
  - Entropy
  - KL Divergence
    - connects suprisal to decomposition
  - Surprisal
- Entropy is total uncertainty
  - Aleatoric + Epistemic
  - Aleatoric
    - Noise in environment
  - Epistemic
    - Noise in model
- We want to reduce epistemic uncertainty
  - Collect trajectories that maximize epistemic bits
- What is learnable anyway?
  - RAIR
  - Epistemic bits -> learnable bits
    - learning progress, rate of epistemic bit decrease
      - hard to compute, noisy
  - you can't fit a line to a curve, some things can't be learned, you should not treat this error as epistemic uncertainty.
- How to compute things
  - Ensembles and decomposition
    - BALD
    - SWAG, distribution over weights
  - Fisher information and KFAC
    - BAIT
- Pre-training, post-training. Mid-training?
  - Expected free energy; goal

<div class=boxed>

**Daily reading: [The Continual Learning Problem](https://jessylin.com/2025/10/20/continual-learning)**

</div>

{{ mathjax() }}
