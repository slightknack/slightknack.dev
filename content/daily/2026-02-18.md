+++
title = "Symbolic grounding"
date = 2026-02-18
+++

After [yesterday's](/daily/2026-02-17/) massive post (and given the two assignments I have to do today) I figured I would write something a little shorter today. A little observation, perhaps.

In the [70s and 80s](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence), people cared a lot about symbolic reasoning; precise relational systems of facts and rules used to derive logical truths. Any relational system is one concerned with resolving constraints. These ideas fell out of favor because of the powerful general "statistical" methods we have today.

From casual usage, I've found that writing code with language models pairs very well with property-based testing. Give the model a list of invariants and constraints, along with types and interface definitions, have it generate a ream of tests using a property-based testing library. 

Verify the tests are correct, then let the model run in a loop against the *symbolically grounded* system until all constraints are satisfied. You can even tell it to keep track of the search tree in a consistent format as it goes, to know what to try next and where to backtrack to.

This turns a fuzzy word problem into an unyielding semantic one; the model acts as a very good prior for performing symbolic search.

We're seeing moves in this direction as newer models employ thousands of little "tool calls" while reasoning, to symbolically ground their reasoning. These systems are trained end-to-end; one day models may be little more than good priors from which to sample belief networks on the fly.

I think datalog or relations generally should be built into programming languages. One language does this well:

<div class=boxed>

**Daily reading: [The Flix Programming Language](https://flix.dev/)**

</div>
